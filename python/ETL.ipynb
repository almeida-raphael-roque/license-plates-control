{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import logging\n",
    "import os\n",
    "import awswrangler as awr\n",
    "\n",
    "# --------------------------------------------------------- EXTRACT\n",
    "\n",
    "class ETL:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.df = None\n",
    "        self.df_ontem = None\n",
    "        self.df_conferencia = None\n",
    "        self.df_cancelamentos = None\n",
    "        self.today = pd.Timestamp.today().date()\n",
    "\n",
    "        if self.today.weekday() == 0:\n",
    "            self.yesterday = self.today - pd.Timedelta(days=3)\n",
    "        else:\n",
    "            self.yesterday = self.today - pd.Timedelta(days=1)\n",
    "\n",
    "        if self.today.weekday() == 0:\n",
    "            self.dbf_yesterday = self.today - pd.Timedelta(days=4)\n",
    "        else:\n",
    "            self.dbf_yesterday = self.today - pd.Timedelta(days=2)\n",
    "    \n",
    "    def extract(self):\n",
    "        try:\n",
    "            xlsx_ontem = rf\"C:\\Users\\raphael.almeida\\Documents\\Processos\\placas_movimentacoes\\bkp_activation\\placas_movimentacoes_{self.yesterday}.xlsx\"\n",
    "            xlsx = rf\"C:\\Users\\raphael.almeida\\Documents\\Processos\\placas_movimentacoes\\bkp_activation\\placas_movimentacoes_{self.today}.xlsx\"\n",
    "            \n",
    "\n",
    "            self.df_ontem = pd.read_excel(xlsx_ontem, engine='openpyxl', sheet_name='ATIVAÇÕES')\n",
    "            self.df = pd.read_excel(xlsx, engine='openpyxl', sheet_name='ATIVAÇÕES')\n",
    "\n",
    "            path = r\"C:\\Users\\raphael.almeida\\Documents\\Processos\\placas_movimentacoes\"\n",
    "            dir_query = os.path.join(path, 'sql', 'listagem_mestra.sql')\n",
    "\n",
    "            with open(dir_query, 'r') as file:\n",
    "                query = file.read()\n",
    "\n",
    "            self.df_conferencia = awr.athena.read_sql_query(query, database='silver')\n",
    "\n",
    "            dir_query = os.path.join(path, 'sql', 'all_boards_CANCELADOS.sql')\n",
    "\n",
    "            with open(dir_query, 'r') as file:\n",
    "                query = file.read()\n",
    "\n",
    "            self.df_cancelamentos = awr.athena.read_sql_query(query, database='silver')\n",
    "            \n",
    "            print(\"Extração concluída com sucesso!\")\n",
    "            return self.df, self.df_ontem\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na extração: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def loading_deactived(self):\n",
    "        try:\n",
    "            set_hoje = set(self.df['chassi'].dropna().unique()) if 'chassi' in self.df.columns else set()\n",
    "\n",
    "            if 'chassi' in self.df_ontem.columns:\n",
    "                desativados_mask = ~self.df_ontem['chassi'].isin(set_hoje)\n",
    "                df_desativados = self.df_ontem.loc[desativados_mask].copy()\n",
    "            else:\n",
    "                df_desativados = pd.DataFrame(columns=self.df_ontem.columns)\n",
    "\n",
    "            xlsx_cancel_path = r\"C:\\Users\\raphael.almeida\\OneDrive - Grupo Unus\\analise de dados - Arquivos em excel\\Relatório de Ativações Placas\\placas_desativadas.xlsx\"\n",
    "            df_desativados.to_excel(xlsx_cancel_path, engine='openpyxl', index=False)\n",
    "            print(\"Arquivo de placas desativadas gerado com sucesso!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao gerar arquivo de placas desativadas: {e}\")\n",
    "\n",
    "    def transform_process(self):\n",
    "        try:\n",
    "            # Normalizar chassi em ambos os dataframes\n",
    "            for d in (self.df, self.df_ontem):\n",
    "                if 'chassi' in d.columns:\n",
    "                    d['chassi'] = d['chassi'].astype(str).str.strip().str.upper()\n",
    "\n",
    "            # Conjuntos para comparação\n",
    "            set_ontem = set(self.df_ontem['chassi'].dropna().unique()) if 'chassi' in self.df_ontem.columns else set()\n",
    "\n",
    "            # Classificar status_beneficio em df\n",
    "             \n",
    "            if 'chassi' in self.df.columns:\n",
    "                mask = self.df['chassi'].isin(set_ontem)\n",
    "                # Só altera status_beneficio para 'ATIVO' ou 'NOVO' nas linhas onde status_beneficio não é diferente de 'ATIVO'\n",
    "                if 'status_beneficio' in self.df.columns:\n",
    "                    mask_to_update = self.df['status_beneficio'].eq('ATIVO') | self.df['status_beneficio'].isna()\n",
    "                else:\n",
    "                    mask_to_update = pd.Series([True] * len(self.df), index=self.df.index)\n",
    "                self.df.loc[mask & mask_to_update, 'status_beneficio'] = 'ATIVO'\n",
    "                self.df.loc[~mask & mask_to_update, 'status_beneficio'] = 'NOVO'\n",
    "            if 'migration_from' not in self.df.columns:\n",
    "                self.df['migration_from'] = 'NULL'\n",
    "\n",
    "            print(\"Transformação de processo concluída com sucesso!\")\n",
    "            return self.df, self.df_ontem\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na transformação de processo: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def transform_movement(self):\n",
    "        try:\n",
    "            df = self.df\n",
    "            df_conf = self.df_conferencia\n",
    "\n",
    "            # Normalização de chaves\n",
    "            if df_conf is not None and not df_conf.empty:\n",
    "                for col in ['chassi', 'beneficio', 'empresa']:\n",
    "                    if col in df_conf.columns:\n",
    "                        if col == 'empresa':\n",
    "                            df_conf[col] = df_conf[col].astype(str).str.strip()\n",
    "                        else:\n",
    "                            df_conf[col] = df_conf[col].astype(str).str.strip().str.upper()\n",
    "                # Garantir datetime\n",
    "                if 'data_ativacao_beneficio' in df_conf.columns:\n",
    "                    df_conf['data_ativacao_beneficio'] = pd.to_datetime(df_conf['data_ativacao_beneficio'], errors='coerce')\n",
    "\n",
    "            # Aplicar apenas nas linhas marcadas como NOVO e que tenham chassi+beneficio\n",
    "            mask_novo = df['status_beneficio'].eq('NOVO')\n",
    "            if (\n",
    "                mask_novo.any()\n",
    "                and {'chassi', 'beneficio'}.issubset(df.columns)\n",
    "                and df_conf is not None\n",
    "                and not df_conf.empty\n",
    "                and {'chassi', 'beneficio', 'data_ativacao_beneficio', 'status_beneficio', 'empresa'}.issubset(df_conf.columns)\n",
    "            ):\n",
    "                df_novos = df.loc[mask_novo, ['chassi', 'beneficio', 'empresa']].copy()\n",
    "                df_novos['beneficio'] = df_novos['beneficio'].astype(str).str.strip().str.upper()\n",
    "                df_novos['empresa'] = df_novos['empresa'].astype(str).str.strip()\n",
    "\n",
    "                # Contagem de registros por par\n",
    "                hist_counts = (\n",
    "                    df_conf\n",
    "                    .groupby(['chassi', 'beneficio'], as_index=False)\n",
    "                    .size()\n",
    "                    .rename(columns={'size': 'hist_count'})\n",
    "                )\n",
    "\n",
    "                # Penúltimo registro por par: ordenar desc e pegar a 2a linha (index 1)\n",
    "                df_conf_sorted = df_conf.sort_values('data_ativacao_beneficio', ascending=False)\n",
    "                penult = (\n",
    "                    df_conf_sorted\n",
    "                    .groupby(['chassi', 'beneficio'], as_index=False)\n",
    "                    .nth(1)\n",
    "                    .reset_index(drop=False)\n",
    "                )\n",
    "                # Selecionar colunas relevantes\n",
    "                penult = penult[['chassi', 'beneficio', 'status_beneficio', 'empresa']].rename(columns={\n",
    "                    'status_beneficio': 'status_penultimo',\n",
    "                    'empresa': 'empresa_penultima'\n",
    "                })\n",
    "\n",
    "                # Merge info de histórico e penúltimo nas linhas NOVO\n",
    "                df_novos = df_novos.merge(hist_counts, on=['chassi', 'beneficio'], how='left')\n",
    "                df_novos = df_novos.merge(penult, on=['chassi', 'beneficio'], how='left')\n",
    "\n",
    "                # Regras:\n",
    "                # - hist_count <= 1 -> permanece NOVO\n",
    "                # - hist_count > 1:\n",
    "                #   - status_penultimo in status_filter_list -> REATIVAÇÃO\n",
    "                #   - else: se empresa_penultima != empresa_atual -> MIGRAÇÃO (migration_from = empresa_penultima)\n",
    "                #           senão -> RENOVAÇÃO\n",
    "                status_filter_list = ['CANCELADO', 'CANCELADA', 'FINALIZADO', 'FINALIZADA', 'NAO RENOVADO']\n",
    "                cond_hist_many = (df_novos['hist_count'].fillna(0) > 1)\n",
    "                cond_penult_cancel = df_novos['status_penultimo'].isin(status_filter_list)\n",
    "                cond_mudou_empresa = (\n",
    "                    df_novos['empresa_penultima'].notna()\n",
    "                    & df_novos['empresa'].notna()\n",
    "                    & (df_novos['empresa_penultima'] != df_novos['empresa'])\n",
    "                )\n",
    "\n",
    "                # Inicial\n",
    "                df_novos['novo_status'] = 'NOVO'\n",
    "                df_novos['novo_migration_from'] = 'NULL'\n",
    "\n",
    "                # REATIVAÇÃO\n",
    "                reativ_mask = cond_hist_many & cond_penult_cancel\n",
    "                df_novos.loc[reativ_mask, 'novo_status'] = 'REATIVAÇÃO'\n",
    "                df_novos.loc[reativ_mask, 'novo_migration_from'] = 'NULL'\n",
    "\n",
    "                # MIGRAÇÃO\n",
    "                migr_mask = cond_hist_many & ~cond_penult_cancel & cond_mudou_empresa\n",
    "                df_novos.loc[migr_mask, 'novo_status'] = 'MIGRAÇÃO'\n",
    "                df_novos.loc[migr_mask, 'novo_migration_from'] = df_novos.loc[migr_mask, 'empresa_penultima']\n",
    "\n",
    "                # RENOVAÇÃO\n",
    "                renov_mask = cond_hist_many & ~cond_penult_cancel & ~cond_mudou_empresa\n",
    "                df_novos.loc[renov_mask, 'novo_status'] = 'RENOVAÇÃO'\n",
    "                df_novos.loc[renov_mask, 'novo_migration_from'] = 'NULL'\n",
    "\n",
    "                # Aplicar de volta no df\n",
    "                df.loc[mask_novo, 'status_beneficio'] = df_novos['novo_status'].values\n",
    "                df.loc[mask_novo, 'migration_from'] = df_novos['novo_migration_from'].values\n",
    "\n",
    "            self.df = df\n",
    "            print(\"Transformação de movimentação concluída com sucesso!\")\n",
    "            return self.df\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na transformação de movimentação: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load(self):\n",
    "        try:\n",
    "            # Executar todo o pipeline se ainda não foi executado\n",
    "            if self.df is None:\n",
    "                print(\"Iniciando pipeline completo...\")\n",
    "                self.extract()\n",
    "                self.loading_deactived()\n",
    "                self.transform_process()\n",
    "                self.transform_movement()\n",
    "            \n",
    "            # Criar diretórios se não existirem\n",
    "            destination_dir = r\"C:\\Users\\raphael.almeida\\OneDrive - Grupo Unus\\analise de dados - Arquivos em excel\\Relatório de Ativações Placas\"\n",
    "            destination_dir2 = r\"C:\\Users\\raphael.almeida\\Documents\\Processos\\placas_movimentacoes\\bkp_activation\"\n",
    "            \n",
    "            os.makedirs(destination_dir, exist_ok=True)\n",
    "            os.makedirs(destination_dir2, exist_ok=True)\n",
    "\n",
    "            file_path = rf\"C:\\Users\\raphael.almeida\\Documents\\Processos\\placas_acompanhamento\\template\\placas_movimentacoes_{self.today}.xlsx\"\n",
    "\n",
    "            destination_path = os.path.join(destination_dir, os.path.basename(file_path))\n",
    "            destination_path2 = os.path.join(destination_dir2, os.path.basename(file_path))\n",
    "\n",
    "            with pd.ExcelWriter(destination_path, engine='openpyxl') as writer:\n",
    "                self.df.to_excel(writer, index=False, sheet_name='ATIVAÇÕES')\n",
    "                self.df_cancelamentos.to_excel(writer, index=False, sheet_name='CANCELAMENTOS')\n",
    "\n",
    "            with pd.ExcelWriter(destination_path2, engine='openpyxl') as writer:\n",
    "                self.df.to_excel(writer, index=False, sheet_name='ATIVAÇÕES')\n",
    "                self.df_cancelamentos.to_excel(writer, index=False, sheet_name='CANCELAMENTOS')\n",
    "\n",
    "            xlsx_rm_path = rf\"C:\\Users\\raphael.almeida\\OneDrive - Grupo Unus\\analise de dados - Arquivos em excel\\Relatório de Ativações Placas\\placas_movimentacoes_{self.dbf_yesterday}.xlsx\"\n",
    "            if os.path.exists(xlsx_rm_path):\n",
    "                os.remove(xlsx_rm_path)\n",
    "\n",
    "            print(\"Pipeline completo executado com sucesso!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro no carregamento: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl = ETL()\n",
    "    etl.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb43875",
   "metadata": {},
   "source": [
    "# EXTRACT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db5c11",
   "metadata": {},
   "source": [
    "## setting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cccde04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import logging\n",
    "import os\n",
    "import awswrangler as awr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae99f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise = None\n",
    "df_anterior = None\n",
    "df_conferencia = None\n",
    "df_cancelamentos = None\n",
    "today = pd.Timestamp.today().date()\n",
    "\n",
    "if today.weekday() == 0:\n",
    "    yesterday = today - pd.Timedelta(days=3)\n",
    "else:\n",
    "    yesterday = today - pd.Timedelta(days=1)\n",
    "\n",
    "if today.weekday() == 0:\n",
    "    dbf_yesterday = today - pd.Timedelta(days=4)\n",
    "else:\n",
    "    dbf_yesterday = today - pd.Timedelta(days=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55494b8d",
   "metadata": {},
   "source": [
    "## extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb40321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_anterior = rf\"C:\\Users\\raphael.almeida\\Documents\\Processos\\placas_movimentacoes\\bkp_activation\\placas_movimentacoes_{yesterday}.xlsx\"\n",
    "xlsx_analise = rf\"C:\\Users\\raphael.almeida\\Documents\\Processos\\placas_movimentacoes\\bkp_activation\\placas_movimentacoes_{today}.xlsx\"\n",
    "\n",
    "df_anterior = pd.read_excel(xlsx_anterior, engine='openpyxl', sheet_name='ATIVAÇÕES')\n",
    "df_analise = pd.read_excel(xlsx_analise, engine='openpyxl', sheet_name='ATIVAÇÕES')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea7b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_analise = set(df_analise['chassi'])\n",
    "set_anterior = set(df_anterior['chassi'])\n",
    "\n",
    "novos_mask = ~df_analise['chassi'].isin(set_anterior)\n",
    "\n",
    "desativados_mask = ~df_anterior['chassi'].isin(set_analise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec262f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de df_anterior: (33374, 18)\n",
      "Shape de df_analise: (33381, 18)\n",
      "Quantidade de novos: 91\n",
      "Quantidade de desativados: 84\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape de df_anterior: {df_anterior.shape}\")\n",
    "print(f\"Shape de df_analise: {df_analise.shape}\")\n",
    "print(f\"Quantidade de novos: {novos_mask.sum()}\")\n",
    "print(f\"Quantidade de desativados: {desativados_mask.sum()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
